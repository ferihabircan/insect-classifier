# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u88_FphGS6tK1zc89ey6ofoPpTRDm6Y9
"""

#@title 1. Setup Environment and Install Libraries
# Install necessary libraries
!pip install -q fastai duckduckgo_search gradio huggingface_hub kaggle

# Import libraries
import os
import fastai
from pathlib import Path
from fastai.vision.all import *
import gradio as gr
from huggingface_hub import notebook_login, push_to_hub_fastai, upload_file, HfApi, create_repo

print("Fastai version:", fastai.__version__)
print("Gradio version:", gr.__version__)

# You can run these in a Colab code cell before re-running cell 2
!rm -rf insect_data
!rm -f insect-identification-from-habitus-images.zip

#@title 2. Download and Prepare Data (A.1) (Improved)

# A.1. Download the data
# Configure Kaggle API
from google.colab import files
import subprocess # For better command execution and checking

if not os.path.exists('/root/.kaggle/kaggle.json'):
    print("Please upload your kaggle.json file.")
    files.upload()
    !mkdir -p ~/.kaggle
    !cp kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    !rm kaggle.json # Clean up the uploaded file from the current directory
else:
    print("Kaggle API key already exists.")

# Define dataset path
dataset_owner_slug = "kmldas"
dataset_name_slug = "insect-identification-from-habitus-images"
dataset_full_slug = f"{dataset_owner_slug}/{dataset_name_slug}"

dataset_path = Path('insect_data')
zip_file_name = f'{dataset_name_slug}.zip' # Correct zip file name
zip_path = Path(zip_file_name)

# Clean up previous attempts if any
if dataset_path.exists():
    print(f"Removing existing dataset directory: {dataset_path}")
    !rm -rf {dataset_path}
if zip_path.exists():
    print(f"Removing existing zip file: {zip_path}")
    !rm -f {zip_path}

print(f"Attempting to download {dataset_full_slug}...")
download_command = ["kaggle", "datasets", "download", "-d", dataset_full_slug, "-p", "."] # Download to current dir

try:
    # Execute the download command
    result = subprocess.run(download_command, capture_output=True, text=True, check=True)
    print("Kaggle download command executed successfully.")
    print(f"Output:\n{result.stdout}")
    if result.stderr:
        print(f"Stderr:\n{result.stderr}")

    if not zip_path.exists():
        print(f"ERROR: Kaggle download command ran, but zip file '{zip_path}' was not found!")
        print("Please check Kaggle for any issues with the dataset or try again later.")
    else:
        print(f"Zip file '{zip_path}' found. Size: {zip_path.stat().st_size / (1024*1024):.2f} MB")
        print("Unzipping dataset...")
        # Ensure the target directory exists before unzipping
        dataset_path.mkdir(parents=True, exist_ok=True)
        unzip_command = ["unzip", "-q", str(zip_path), "-d", str(dataset_path)]
        subprocess.run(unzip_command, check=True)
        print("Dataset unzipped successfully.")
        !rm {zip_path} # Clean up the zip file
        print("Zip file removed.")

        # A.1.1. Inspect the data layout
        data_dir = dataset_path / 'database' # The actual images are in 'database' subdirectory
        if not data_dir.exists():
            # Sometimes the unzip might create an extra top-level folder matching the zip name
            # e.g. insect_data/insect-identification-from-habitus-images/database
            # Let's check for that common pattern
            possible_intermediate_dir = dataset_path / dataset_name_slug
            if (possible_intermediate_dir / 'database').exists():
                data_dir = possible_intermediate_dir / 'database'
                print(f"Adjusted data_dir to: {data_dir}")
            else:
                 print(f"ERROR: Data directory '{data_dir}' not found after unzipping!")
                 # List contents of dataset_path to help debug
                 print(f"Contents of '{dataset_path}': {os.listdir(dataset_path)}")


        print(f"\nData directory: {data_dir}")
        if data_dir.exists() and data_dir.is_dir():
            print(f"Number of class folders (species): {len(os.listdir(data_dir))}")
            print("First 5 class folders (GBIF IDs):")
            for i, folder_name in enumerate(os.listdir(data_dir)):
                if i < 5:
                    folder_path = data_dir / folder_name
                    if folder_path.is_dir():
                         print(f"- {folder_name} (contains {len(os.listdir(folder_path))} images)")
                else:
                    break
            fnames = get_image_files(data_dir) # Ensure get_image_files is available or import it
            print(f"\nTotal number of images: {len(fnames)}")
            if fnames:
                print(f"Example image path: {fnames[0]}")
        else:
            print(f"ERROR: Final data directory '{data_dir}' does not exist or is not a directory.")


except subprocess.CalledProcessError as e:
    print(f"ERROR during Kaggle download or unzip: {e}")
    print(f"Stdout: {e.stdout}")
    print(f"Stderr: {e.stderr}")
    print("Please ensure your Kaggle API token is correctly set up and the dataset slug is correct.")
    print("Also, check for any temporary issues on Kaggle's side.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")


# A.1.2. Decide how to create the datablock
# (This part is fine as conceptual comment)
# The images are in folders named by their class (GBIF ID).
# This is a standard "ImageFolder" type layout.
# We will use `ImageBlock` for the images and `CategoryBlock` for the labels.
# Labels can be extracted from the parent folder name.

#@title 3. Create DataBlock and DataLoaders (A.2)

# A.2.1 Define the blocks
# A.2.2 Define the means of getting data into DataBlock (get_items)
# A.2.3 Define how to get the attributes (get_y)
# A.2.4 Define data transformations (item_tfms, batch_tfms with Presizing)

# Presizing strategy:
# 1. Resize images to a larger size (e.g., 460x460) - item_tfms
# 2. Apply augmentations on batches, then crop to final size (e.g., 224x224) - batch_tfms

# Set a seed for reproducibility
set_seed(42, reproducible=True)

# For a large number of classes and potentially imbalanced data,
# a larger batch size might be beneficial if memory allows,
# but we'll start with a common default.
# Reduce batch_size if you encounter CUDA out-of-memory errors.

# A.2. Create the DataBlock and dataloaders
insects_datablock = DataBlock(
    blocks=(ImageBlock, CategoryBlock), # Input is an image, output is a category
    get_items=get_image_files,         # How to find all the items
    splitter=RandomSplitter(valid_pct=0.2, seed=42), # How to split into train/valid
    get_y=parent_label,                # How to get the label (from parent folder name)
    item_tfms=Resize(460),             # Presizing: resize individual images
    batch_tfms=aug_transforms(size=224, min_scale=0.75) # Augment and crop batches
)

# Create DataLoaders
# If you run out of GPU memory, reduce batch_size
batch_size = 128 # Or 64, or 256. Experiment!
dls = insects_datablock.dataloaders(data_dir, bs=batch_size)

print("\nDataLoaders created.")
print(f"Number of classes (vocab size): {dls.c}")
print(f"Class names (first 10): {dls.vocab[:10]}")
print(f"Training dataset size: {len(dls.train_ds)}")
print(f"Validation dataset size: {len(dls.valid_ds)}")

#@title 4. Inspect the DataBlock via DataLoader (A.3)

# A.3.1 Show batch: dataloader.show_batch()
print("Showing a batch of data:")
dls.show_batch(max_n=9, figsize=(9,9))
plt.show()

# A.3.2 Check the labels (already done by printing dls.vocab and dls.c)
print(f"\nLabels (Vocabulary): {dls.vocab}")
print(f"Number of unique labels: {len(dls.vocab)}")

# A.3.3 Summarize the DataBlock: dataloader.summary(path/data)
# Note: dls.summary() can be quite verbose.
# It shows how a single batch is processed through the transformations.
# We'll print a portion or skip if too long for this interactive demo.
print("\nDataBlock Summary (this can be long, showing a conceptual representation):")
# dls.summary(data_dir) # This can be very verbose, especially with many files.
# Instead, let's look at the structure of a single batch:
x, y = dls.one_batch()
print(f"Shape of one batch of images (x): {x.shape}") # (batch_size, channels, height, width)
print(f"Shape of one batch of labels (y): {y.shape}") # (batch_size)
print(f"Data type of x: {x.dtype}")
print(f"Data type of y: {y.dtype}")
print(f"Example labels from batch: {dls.vocab[y[:5]]}")

#@title 5. Train a Simple Model (Benchmark) (A.4)

# A.4.1 Create a benchmark
# We'll use a common pretrained model like resnet34
# We'll train for a few epochs to establish a baseline.
# Using error_rate as metric (1 - accuracy)

print("\nTraining a benchmark model...")
# `vision_learner` automatically uses pretrained weights and adds a new head.
# It also freezes the backbone by default for fine_tune.
learn_benchmark = vision_learner(dls, resnet34, metrics=error_rate)

# Train for a small number of epochs for the benchmark
# The `fine_tune` method first trains the new head (frozen backbone),
# then unfreezes and trains the whole network for more epochs.
print("Benchmarking with fine_tune (1 epoch head, 3 epochs unfreeze):")
learn_benchmark.fine_tune(epochs=3, base_lr=1e-3) # 1 epoch for head, 3 for full model

# A.4.2 Interpret the model (partially covered by metrics)
# We'll look at the confusion matrix.

# A.4.3 Confusion matrix
print("\nPlotting confusion matrix for the benchmark model...")
# Due to the large number of classes (291), the full confusion matrix
# will be very large and hard to interpret visually.
# We can show it, but it's more useful for models with fewer classes.
# interp = ClassificationInterpretation.from_learner(learn_benchmark)
# interp.plot_confusion_matrix(figsize=(12,12), dpi=60) # This will be huge!
# plt.show()
# print("Top losses for benchmark model:")
# interp.plot_top_losses(9, figsize=(15,10))
# plt.show()

print("Benchmark training complete. Metrics are printed above.")
print("Note: Full confusion matrix plotting is commented out due to the high number of classes.")
print("It's better to look at overall metrics like error_rate/accuracy for this many classes.")

interp = ClassificationInterpretation.from_learner(learn_benchmark)
interp.plot_confusion_matrix(figsize=(12,12), dpi=60) # This will be huge!
plt.show()
print("Top losses for benchmark model:")
interp.plot_top_losses(9, figsize=(15,10))
plt.show()

#@title 6. Advanced Training Techniques (B) (Corrected LR Selection)

# B.1. Learning Rate Finder (B.1.1, B.1.2, B.2)
print("\n--- Advanced Training Techniques ---")
print("\nB.1 & B.2: Learning Rate Finder")

# Create a new learner instance for advanced training to start fresh
# Or you can continue with the benchmark learner if you prefer.
# For demonstration, let's make a new one.
learn = vision_learner(dls, resnet34, metrics=error_rate)

# Run the Learning Rate Finder
# The finder algorithm (B.2.1-B.2.5) is implemented in lr_find()
lr_finder_result = learn.lr_find(show_plot=True) # Set show_plot=True to see the graph
# The lr_find() function itself will print the plot. plt.show() might not be needed after it.

print(f"LR Finder results: {lr_finder_result}")

# Corrected LR selection:
# Common choices:
# - learn.lr_find() often returns an object with attributes like 'steepest' or 'valley'.
# - 'steepest' is generally a good pick.
# - 'valley' is where loss is minimal; we usually pick a rate *before* this, e.g., valley/10.

if hasattr(lr_finder_result, 'steepest') and lr_finder_result.steepest is not None:
    suggested_lr = lr_finder_result.steepest
    print(f"Using 'steepest' learning rate: {suggested_lr:.2e}")
elif hasattr(lr_finder_result, 'valley') and lr_finder_result.valley is not None:
    suggested_lr = lr_finder_result.valley / 10
    print(f"Using 'valley'/10 learning rate: {suggested_lr:.2e}")
else:
    # Fallback if neither steepest nor valley is found (should be rare)
    print("LR Finder did not suggest 'steepest' or 'valley'. Plotting to help manual selection.")
    print("Defaulting to 1e-3. PLEASE INSPECT THE PLOT AND ADJUST IF NECESSARY.")
    suggested_lr = 1e-3
    # learn.recorder.plot_lr_find() # lr_find with show_plot=True should already do this.
    # plt.show()

print(f"Chosen learning rate after LR finder: {suggested_lr:.2e}")


# B.3. Transfer Learning (already happening with vision_learner)
# ... (rest of the cell remains the same) ...
print("\nB.3: Transfer Learning")
print("`vision_learner` with a pretrained model (e.g., resnet34) inherently uses transfer learning.")
print("`fine_tune` method handles freezing the backbone and training the head, then unfreezing and training all layers.")

# B.4. Discriminative Learning Rates
# ... (rest of the cell remains the same) ...
print("\nB.4: Discriminative Learning Rates")
print("`fine_tune` applies discriminative learning rates automatically.")
print("Early layers get smaller LRs, later/custom layers get larger LRs.")

# B.5. Deciding the Number of Training Epochs
# ... (rest of the cell remains the same) ...
num_epochs_head = 2
num_epochs_full = 8
print(f"\nB.5: Deciding Number of Training Epochs")
print(f"Will train head for {num_epochs_head} epochs, then full model for {num_epochs_full} epochs.")

# B.6. Model Capacity & Mixed Precision
# ... (rest of the cell remains the same) ...
print("\nB.6: Model Capacity and Mixed Precision")
print("Current model: resnet34. For higher capacity, consider resnet50, resnet101, etc.")
print("If using a larger model, you might need to reduce batch_size.")
print("Applying mixed precision training (.to_fp16()).")
learn.to_fp16()

# B.7. Weight Initialization (EXTRA NOTES)
# ... (rest of the cell remains the same) ...
print("\nB.7: Weight Initialization")
print("Using a pretrained model, so backbone weights are already initialized.")
print("Fastai handles initialization of the new classification head (typically Kaiming for ReLU).")


# --- Perform the training with advanced techniques ---
print(f"\nStarting fine-tuning with suggested LR ({suggested_lr:.2e}), {num_epochs_head} head epochs, {num_epochs_full} full epochs...")
learn.fine_tune(epochs=num_epochs_full,
                base_lr=suggested_lr,
                freeze_epochs=num_epochs_head)

print("\nAdvanced training complete.")

# Interpretation after advanced training
print("\nPlotting confusion matrix for the advanced model (if feasible)...")
# interp_advanced = ClassificationInterpretation.from_learner(learn)
# interp_advanced.plot_confusion_matrix(figsize=(12,12), dpi=60)
# plt.show()
# print("Top losses for advanced model:")
# interp_advanced.plot_top_losses(9, figsize=(15,10))
# plt.show()
print("Advanced model interpretation: Check printed metrics. Full CM plotting still commented out.")

#@title 7. Export the Model
model_export_path = Path("insect_classifier_model.pkl")
learn.export(model_export_path)
print(f"Model exported to {model_export_path}")
print(f"File size: {os.path.getsize(model_export_path)/1e6:.2f} MB")

# You can download it from Colab's file browser on the left.
# Or use:
# from google.colab import files
# files.download(model_export_path)

#@title 8. Create Gradio Interface

# Load the exported model (simulates how it would be loaded in app.py)
learn_inf = load_learner(model_export_path)
labels = learn_inf.dls.vocab

def predict_insect(image):
    """
    Predicts the insect species from an image.
    `image` is a PIL.Image object.
    Returns a dictionary of labels and probabilities.
    """
    pred, pred_idx, probs = learn_inf.predict(image)
    # Ensure probs are float for JSON serialization if needed (Gradio handles it)
    return {labels[i]: float(probs[i]) for i in range(len(labels))}


# Create Gradio interface
# Inputs: Image upload
# Outputs: Label (shows top N classes with probabilities)
gr_interface = gr.Interface(
    fn=predict_insect,
    inputs=gr.Image(type="pil", label="Upload Insect Image"),
    outputs=gr.Label(num_top_classes=5, label="Predictions"),
    title="Insect Species Classifier",
    description="Upload an image of an insect to classify its species. Based on 'Insect identification from habitus images' dataset and trained with Fastai.",
    examples=[str(fnames[i]) for i in random.sample(range(len(fnames)), k=min(3, len(fnames)))] # provide 3 random examples from dataset
)

print("\nGradio interface defined. Launching in a new cell...")

#@title Launch Gradio Interface (locally in Colab)
# This will run the Gradio app. You'll get a public link if share=True,
# or it will run inline/on a local link within Colab.
gr_interface.launch(share=True, debug=True) # share=True gives a public link (expires)

#@title 9. Deploy to Hugging Face Spaces

# --- IMPORTANT ---
# 1. You need a Hugging Face account.
# 2. You need a Hugging Face API token with WRITE access.
#    Go to https://huggingface.co/settings/tokens and create one.

# Log in to Hugging Face Hub
print("Login to Hugging Face Hub (requires a User Access Token with write permission):")
notebook_login() # Paste your HF token here when prompted

# Define your Hugging Face username and the desired Space name
# (Replace 'YOUR-HF-USERNAME' and 'YOUR-SPACE-NAME')
hf_username = "evasvrski" # <--- CHANGE THIS
space_name = "insect-classifier" # <--- You can change this

# Create the repo on Hugging Face Hub (if it doesn't exist)
repo_id = f"{hf_username}/{space_name}"

try:
    create_repo(repo_id, repo_type="space", space_sdk="gradio", exist_ok=True)
    print(f"Space repository {repo_id} created or already exists.")
except Exception as e:
    print(f"Error creating repo: {e}")
    print("Please ensure you have the correct username and the token has write permissions.")
    # Stop here if repo creation fails

# --- Create app.py for Gradio on Hugging Face ---
app_py_content = f"""
import gradio as gr
from fastai.vision.all import *
import timm # Often a fastai vision dependency, good to include

# Model path in the Hugging Face Space environment
model_path = Path("insect_classifier_model.pkl")

# Load the learner
try:
    learn = load_learner(model_path)
    labels = learn.dls.vocab
except Exception as e:
    print(f"Error loading model: {{e}}")
    learn = None # Handle model loading failure gracefully in Gradio if needed
    labels = ["Error loading model"]


def predict_insect(image):
    if learn is None:
        return {{"Error": 1.0, "Could not load model": 1.0}}
    try:
        pred, pred_idx, probs = learn.predict(image)
        return {{labels[i]: float(probs[i]) for i in range(len(labels))}}
    except Exception as e:
        print(f"Error during prediction: {{e}}")
        return {{"Error": 1.0, "Prediction failed": 1.0}}

# Create and launch Gradio interface
gr_interface = gr.Interface(
    fn=predict_insect,
    inputs=gr.Image(type="pil", label="Upload Insect Image"),
    outputs=gr.Label(num_top_classes=5, label="Predictions"),
    title="Insect Species Classifier",
    description="Upload an image of an insect to classify its species. Model trained with Fastai.",
    # Add example images if you upload them to the Space
    # examples=["example1.jpg", "example2.jpg"] # You'd need to upload these
)

gr_interface.launch()
"""

with open("app.py", "w") as f:
    f.write(app_py_content)
print("app.py created.")

# --- Create requirements.txt ---
# List libraries needed by your app.py on Hugging Face
# Ensure versions are compatible, or let HF resolve them.
# fastai includes torch, torchvision, etc.
# timm is often needed for vision_learner architectures.
requirements_txt_content = f"""
fastai
timm
gradio
# Potentially specific versions if needed, e.g.:
# torch==1.13.1
# torchvision==0.14.1
"""
with open("requirements.txt", "w") as f:
    f.write(requirements_txt_content)
print("requirements.txt created.")


# --- Upload files to Hugging Face Space ---
# Using HfApi for more control
api = HfApi()

try:
    print(f"Uploading model ({model_export_path.name}) to {repo_id}...")
    api.upload_file(
        path_or_fileobj=str(model_export_path),
        path_in_repo=model_export_path.name,
        repo_id=repo_id,
        repo_type="space"
    )
    print("Model uploaded.")

    print(f"Uploading app.py to {repo_id}...")
    api.upload_file(
        path_or_fileobj="app.py",
        path_in_repo="app.py",
        repo_id=repo_id,
        repo_type="space"
    )
    print("app.py uploaded.")

    print(f"Uploading requirements.txt to {repo_id}...")
    api.upload_file(
        path_or_fileobj="requirements.txt",
        path_in_repo="requirements.txt",
        repo_id=repo_id,
        repo_type="space"
    )
    print("requirements.txt uploaded.")

    # You might want to upload a few example images too
    # for f_example in fnames[:3]:
    #     api.upload_file(
    #         path_or_fileobj=str(f_example),
    #         path_in_repo=Path(f_example).name, # Store in root of space repo
    #         repo_id=repo_id,
    #         repo_type="space"
    #     )
    # print("Example images uploaded.")


    print(f"\nSuccessfully uploaded files to Hugging Face Space: https://huggingface.co/spaces/{repo_id}")
    print("The Space should start building. Check the 'Logs' tab on your Space page for progress.")

except Exception as e:
    print(f"An error occurred during upload: {e}")
    print("Ensure your HF_TOKEN is correctly set up with write permissions,")
    print("and your hf_username and space_name are correct.")